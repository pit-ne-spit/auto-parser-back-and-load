# Парсер CHE168.COM - Документация проекта

## Глобальная задача

Верхнеуровневая работа этого проекта должна выглядеть так: один раз мы забираем данные (объявления автомобилей) при помощи `/offers`, а затем при помощи `/change_id` и `/changes` на ежедневной основе забираем обновления в виде новых объявлений или изменений старых.

У нас должна быть своя БД. Ожидается, что первоначальная загрузка это около 250 тысяч объявлений. Ежедневное обновление так или иначе затрагивает около 15000 объявлений. Логика такая, что мы забираем и складываем в одну таблицу абсолютно все данные получаемые из API в сыром виде, а затем при помощи какого-то пост процессора нормализуем их и кладем в другую таблицу и из этой таблицы уже данные пойдут в наш публичный API.

Все это будет работать на VPS в Docker контейнере. Получается, что фронтенда не будет, а будет два модуля: первый модуль это разовое скачивание всех объявлений и их обновление, а второй модуль это нормализация и обогащение данных. А в конце будет API которое будет отдавать данные. Для API нужно будет предусмотреть генерацию токенов и их сохранение в БД.

**Развертывание:** Первоначальная загрузка будет выполняться локально в локальную БД, затем БД будет реплицирована на VPS. Обновления данных будут выполняться уже на VPS.

---

## Ответы на уточняющие вопросы

### 1. База данных

- **СУБД:** PostgreSQL
- **Индексы:** Да, будут нужны. По каким столбцам определим позднее
- **Версионность:** Да, версионность будет нужна. Объявления могут быть актуальными сегодня, но терять свою актуальность завтра. Это значит, что хранить их по прежнему надо, но отдавать в ответе API не будем

### 2. Структура данных

- **Хранение сырых данных:** Данных слишком много, распарсить в колонки навряд ли получится. Думаю, они будут занимать меньше места если будем хранить в JSON, а потом при постпроцессинге, уже разложим по колонкам в новую таблицу.
  
  **Структура ответа API (см. example.txt):**
  - `meta` - метаданные (page, next_page, limit) - **НЕ сохраняем в БД**
  - `result[]` - массив объявлений:
    - `id` - внутренний ID → **отдельная колонка**
    - `inner_id` - ID на платформе → **отдельная колонка**
    - `change_type` - тип изменения → **отдельная колонка**
    - `created_at` - дата создания → **отдельная колонка**
    - `data` - полные данные объявления → **хранить как JSON**
  
  **Дополнительные колонки:**
  - `first_loaded_at` (timestamp) - когда мы впервые получили это объявление из API
  - `last_updated_at` (timestamp) - когда мы последний раз обновляли эту запись
  - `source` (varchar) - откуда пришли данные: "initial_load" (первая загрузка) или "daily_update" (ежедневное обновление)
  - `active_status` (integer) - статус объявления:
    - `0` - объявление действующее (активное)
    - `1` - объявление неактивное (удалено/снято)
  - `is_processed` (boolean, default false) - флаг обработки нормализатором:
    - `false` - нужно обработать нормализатором
    - `true` - уже обработано нормализатором

- **История изменений:** Историю хранить не надо, их надо будет просто обновлять

- **Удаленные объявления:** Используем `active_status = 1` для неактивных объявлений

### 3. Нормализация данных

- **Что такое нормализация:** Это мы обсудим на этапе когда будем реализовывать нормализацию

### 4. Публичное API

- **Эндпоинты:**
  - Список объявлений с фильтрацией и пагинацией
  - Детали конкретного объявления
  
- **Авторизация:** Авторизация для всех эндпоинтов, публичных не будет

- **Rate limiting:** Лимиты для всех токенов будут одинаковые, чтобы хватило для работы фронтенда

### 5. Развертывание и инфраструктура

- **Частота обновлений:** Обновления надо будет запускать один раз в сутки

- **Запуск процессов:** Использовать Cron на VPS для запуска ежедневных процессов (загрузка данных и нормализация)

- **Логирование:** Нужно только логирование

- **Уведомления об ошибках:** Пусть ошибки будут в логах

### 6. Первоначальная загрузка

- **Обработка 250k объявлений:** Пакетами с пагинацией
  
  **Параметры пагинации:**
  - Лимит на страницу всегда равен 20, его изменить невозможно
  - Общее количество страниц неизвестно
  - Ориентироваться необходимо на `next_page` в `meta`
  - Если `next_page: null`, то следующей страницы нет и можно останавливаться

- **Прогресс:** Прогресс бар точно нужен (консольный текстовый формат)

- **Обработка ошибок:** При ошибках во время загрузки нужно повторять попытку **20 раз в течение 4 часов** (интервал **12 минут** между попытками). Если не получится после всех попыток, делать в логах отметку и продолжать далее с последней успешной страницы

---

## Ответы на дополнительные вопросы

### 1. Таблица raw_data - дополнительные колонки

**Ответ:** Да, все эти колонки нужны:
- `first_loaded_at` (timestamp) - когда мы впервые получили это объявление из API
- `last_updated_at` (timestamp) - когда мы последний раз обновляли эту запись
- `source` (varchar) - откуда пришли данные: "initial_load" (первая загрузка) или "daily_update" (ежедневное обновление)

---

### 2. Обработка разных типов изменений из API

**Ответ:**
- `change_type: "added"` → создаем новую запись в БД с `active_status = 0` и `is_processed = false`
- `change_type: "changed"` → **обновляем существующую запись** по `inner_id`:
  - Делаем merge измененных полей в JSON `data` (обновляем только измененные поля, не перезаписывая весь JSON)
  - Обновляем `change_type`
  - Обновляем `created_at` (дата из API)
  - Обновляем `last_updated_at` (текущее время)
  - Устанавливаем `is_processed = false` (чтобы нормализатор обработал изменения)
  - `active_status` остается прежним (или обновляется если изменился)
- `change_type: "removed"` → **обновляем существующую запись** по `inner_id`:
  - Устанавливаем `active_status = 1`
  - Обновляем `change_type`
  - Обновляем `last_updated_at` (текущее время)
  - Устанавливаем `is_processed = false` (чтобы нормализатор обработал изменения)

**Важно:** `inner_id` должен быть уникальным всегда. Если записи с таким `inner_id` нет в БД при "changed" или "removed", создаем новую запись (edge case из вопроса 1).

**Примечание:** При "changed" в `data` могут быть только измененные поля (например, `{"new_price": 7500}`). В этом случае делаем merge с существующим JSON, обновляя только эти поля.

---

### 3. Уникальность записей

**Ответ:** Уникальность определяется по `inner_id` - это уникальный параметр. Одно объявление = одна запись в БД.

---

### 4. Создание записей в таблице processed_data

**Ответ:** Отдельным процессом - загрузили все в raw_data, потом отдельно запускаем процесс нормализации.

**Связь таблиц:** Таблицы будут связываться по `inner_id` (foreign key не обязателен, достаточно использовать inner_id для связи).

---

### 5. Токены для API

**Ответ:**
- **Формат токена:** 16-значный UUID-подобный код (например, `a1b2c3d4e5f6g7h8`)
- **Алгоритм генерации:** Использовать UUID-подобный формат (16 символов: буквы и цифры)
- **Колонки в таблице токенов:**
  - `token` (varchar) - сам токен
  - `created_at` (timestamp) - дата создания
  - `expires_at` (timestamp, nullable) - срок действия (или NULL если бессрочный)
  - `description` (varchar, nullable) - описание/название токена
  - `last_used_at` (timestamp, nullable) - дата последнего использования

---

### 6. Логирование

**Ответ:**
- **Формат:** Обычный текстовый файл
- **Уровни:** Да, нужны разные уровни (INFO, WARNING, ERROR)
- **Ротация:** Удалять логи старше одной недели
- **Язык:** Логи на английском языке

**Пример формата логов:**
```
[INFO] Starting page 1 loading
[INFO] Loaded 100 offers
[ERROR] Error loading page 5
```

---

### 7. Docker для локальной разработки

**Ответ:** Нет, локально Docker не нужен. PostgreSQL устанавливается локально, модули и API запускаются локально.

---

### 8. Репликация БД на VPS

**Ответ:** Использовать PostgreSQL dump/restore:
- На локальной машине: `pg_dump` для создания дампа БД
- На VPS: `pg_restore` для восстановления БД из дампа

---

## Дополнительная таблица: operations_log

**Требование:** Нужна отдельная таблица для отслеживания операций обновления данных.

**Назначение:** Записывать информацию о прошедших операциях обновления данных.

**Колонки:**
- `id` (serial, primary key) - уникальный ID операции
- `operation_type` (varchar) - тип операции:
  - "data_fetch" - получение данных из API
  - "normalization" - нормализация данных
- `started_at` (timestamp) - дата и время начала операции
- `duration` (interval или integer в секундах) - продолжительность операции
- `status` (varchar) - статус операции:
  - "OK" - операция завершена успешно (не было ни одной ошибки)
  - "ERROR" - в процессе операции были ошибки
- `details` (text, nullable) - дополнительная информация об операции (опционально)

**Пример использования:**
- Запись: `operation_type="data_fetch"`, `started_at="2025-02-06 10:00:00"`, `duration=3600`, `status="OK"`
- Запись: `operation_type="normalization"`, `started_at="2025-02-06 11:00:00"`, `duration=1800`, `status="ERROR"`

---

### Таблица: sync_state (состояние синхронизации)

**Требование:** Нужна отдельная таблица для хранения последнего успешного обновления.

**Назначение:** Отслеживать прогресс обработки изменений, чтобы при следующем запуске продолжать с нужного места.

**Колонки:**
- `id` (serial, primary key) - уникальный ID записи
- `last_successful_date` (date) - дата последнего успешного обновления
- `last_change_id` (bigint, nullable) - последний обработанный change_id (для продолжения с нужного места)
- `updated_at` (timestamp) - когда была обновлена эта запись

**Логика работы:**
- При успешном завершении ежедневного обновления обновляем запись с текущей датой и последним change_id
- При следующем запуске берем `last_successful_date + 1 день` для получения change_id через `/change_id?date=...`
- Если записей нет (первый запуск), начинаем с сегодняшней даты

**Индексы:**
- По `last_successful_date` (для быстрого поиска последней даты)

**Пример использования:**
- Запись: `last_successful_date="2025-02-05"`, `last_change_id=123456789`, `updated_at="2025-02-05 23:59:59"`
- При следующем запуске (2025-02-06): берем дату `2025-02-06`, получаем change_id через `/change_id?date=2025-02-06`

---

## Конфигурация проекта

### Файл .env (чувствительные данные)

**Содержимое:**
- `DB_HOST` - хост PostgreSQL
- `DB_PORT` - порт PostgreSQL
- `DB_USER` - пользователь PostgreSQL
- `DB_PASSWORD` - пароль PostgreSQL
- `DB_NAME` - название базы данных
- `CHE168_API_KEY` - API ключ для CHE168.COM
- `CHE168_ACCESS_NAME` - access_name для CHE168.COM (autobase)
- `SECRET_KEY` - секретный ключ для генерации токенов (опционально)

### Файл config (технические настройки)

**Содержимое:**
- Настройки логирования (путь к файлам, уровни, формат, ротация)
- Настройки rate limiting (лимиты запросов)
- Настройки пагинации (размер страницы по умолчанию)
- Размеры батчей для обработки (200 записей для нормализации)
- Интервалы повторных попыток (12 минут между попытками, максимум 20 попыток)
- Таймауты для запросов к API
- Другие технические параметры

**Формат:** YAML файл (например, `config.yaml`)

---


## Итоговая структура базы данных

### Таблица: raw_data (сырые данные из API)

**Колонки:**
- `id` (serial, primary key) - внутренний ID записи
- `inner_id` (varchar, unique) - ID объявления на платформе CHE168.COM (уникальный)
- `change_type` (varchar) - тип изменения: "added", "changed", "removed"
- `created_at` (timestamp) - дата создания записи (из API)
- `data` (jsonb) - полные данные объявления в формате JSON
- `first_loaded_at` (timestamp) - когда мы впервые получили это объявление из API
- `last_updated_at` (timestamp) - когда мы последний раз обновляли эту запись
- `source` (varchar) - источник данных: "initial_load" или "daily_update"
- `active_status` (integer) - статус объявления: 0 (активное) или 1 (неактивное)
- `is_processed` (boolean, default false) - флаг обработки нормализатором: false (нужно обработать), true (уже обработано)

**Индексы:** (определим позднее)
- По `inner_id` (уже unique)
- По `active_status` (для фильтрации активных объявлений)
- По `is_processed` (для выборки записей для нормализации)
- По `source` (опционально)
- По `last_updated_at` (опционально)

---

### Таблица: processed_data (нормализованные данные)

**Колонки:** (структура будет определена на этапе реализации нормализации)

**Связь с raw_data:** По `inner_id` (без foreign key, просто по значению)

---

### Таблица: api_tokens (токены для доступа к API)

**Колонки:**
- `id` (serial, primary key) - уникальный ID токена
- `token` (varchar, unique) - сам токен (16-значный код)
- `created_at` (timestamp) - дата создания токена
- `expires_at` (timestamp, nullable) - срок действия токена (NULL если бессрочный)
- `description` (varchar, nullable) - описание/название токена
- `last_used_at` (timestamp, nullable) - дата последнего использования токена

**Индексы:**
- По `token` (уже unique)

**Инициализация:** При создании проекта необходимо создать 20 токенов сразу. Токены будут выдаваться вручную, отмечать кому выдан будет пользователь.

---

### Таблица: operations_log (логи операций)

**Колонки:**
- `id` (serial, primary key) - уникальный ID операции
- `operation_type` (varchar) - тип операции: "data_fetch" или "normalization"
- `started_at` (timestamp) - дата и время начала операции
- `duration` (integer) - продолжительность операции в секундах
- `status` (varchar) - статус операции: "OK" или "ERROR"
- `details` (text, nullable) - дополнительная информация об операции

**Индексы:**
- По `operation_type`
- По `started_at` (для сортировки по дате)
- По `status` (для фильтрации по статусу)

---

## Ответы на дополнительные вопросы (часть 2)

### 1. Обработка edge cases при обновлениях

**Ответ:**
- Если приходит `change_type: "changed"` для объявления с `inner_id`, которого нет в БД → **создавать новую запись** с `active_status = 0` и `is_processed = false`
- Если приходит `change_type: "removed"` для объявления, которого нет в БД → **создавать новую запись** с `active_status = 1` и `is_processed = false`
- Дубликаты при первоначальной загрузке → **не обрабатывать** (игнорировать, так как inner_id уникален и запись уже существует)

---

### 2. Процесс нормализации данных

**Ответ:**
- **Частота:** Процесс нормализации запускается ежедневно после этапа обновления данных
- **Обработка:** Нормализует только обновленные данные (где `is_processed = false`)
- **При падении:** Если процесс упал, продолжать с того места где упал (записи остаются с `is_processed = false`)
- **Повторная нормализация:** Возможна через флаг `is_processed` в raw_data
- **Определение записей:** По колонке `is_processed` (false = нужно нормализовать)
- **Батчи:** Обрабатывать батчами по 200 записей
- **Транзакционность:** Транзакция на весь батч - если ошибка, откатывать весь батч
- **Обновление флага:** `is_processed = true` устанавливается в конце батча после успешной обработки всех 200 записей
- **Ошибки:** Если запись не обработалась - оставлять `is_processed = false` и логировать ошибку

**Логика работы:**
- После обновления raw_data проставляем `is_processed = false` для измененных записей
- Процесс нормализации обрабатывает записи где `is_processed = false` батчами по 200
- После успешной обработки всего батча проставляем `is_processed = true` для всех записей батча
- Если ошибка в батче - откатываем транзакцию, все записи остаются с `is_processed = false`

---

### 3. Обновление processed_data при изменении raw_data

**Ответ:**
- Если объявление в raw_data обновилось → проставить `is_processed = false`, чтобы нормализатор подхватил его при следующем проходе
- Если объявление стало неактивным (`active_status = 1`) → пометить его как неактивное в processed_data
- **Версионность:** Не нужна, просто обновляем существующую запись

---

### 4. API - детали реализации

**Ответ:** Реализацию API обсудим позднее. Детали будут определены на этапе разработки API.

---

### 5. Обработка ошибок и недоступность API CHE168

**Ответ:**
- **Повторные попытки:** Если API CHE168 недоступен - повторять **20 раз в течение 4 часов** (интервал **12 минут** между попытками) - это относится как к первоначальной загрузке, так и к ежедневным обновлениям
- **Логирование ошибок:** Да, записывать в operations_log статус "ERROR" если API недоступен
- **Прерванная загрузка:** Если первоначальная загрузка прервалась - **продолжать с последней страницы**

---

### 6. Управление токенами

**Ответ:**
- **Админка:** Не нужна, никаких админок не требуется
- **Создание токенов:** На этапе создания проекта создать **20 токенов** сразу
- **Управление:** Токены будут выдаваться вручную, отмечать кому выдан будет пользователь

---

### 7. Обработка больших объемов данных

**Ответ:**
- **Батчи:** Обрабатывать батчами по **200 записей** для лучшего контроля процесса
- **Транзакционность:** (не уточнено, определим при реализации)
- **Очередь:** (не уточнено, определим при реализации)

---

### 8. Статистика и мониторинг

**Ответ:**
- **Статистика по операциям:** Да, нужна (сколько объявлений загружено, сколько обработано, сколько ошибок)
- **Эндпоинт API для статистики:** (не уточнено, определим при реализации API)
- **Статистика использования токенов:** Не нужна пока

---

### 9. Структура проекта и модули

**Ответ:** Определить на свое усмотрение. Использовать стэк актуальный на 2026 год. **Акцент на быстродействие.**

**Рекомендации:**
- **API фреймворк:** FastAPI (быстрый, современный, актуальный на 2026 год)
- **ORM/БД:** SQLAlchemy + asyncpg для асинхронной работы с PostgreSQL
- **Миграции:** Alembic для управления миграциями БД
- **Структура:** Определить при реализации

---

### 10. Конфигурация и настройки

**Ответ:**
- **Чувствительные данные:** Хранить в `.env` файле (пароли, ключи API, токены)
- **Остальные настройки:** Хранить в отдельном config файле (например, `config.yaml` или `config.py`)

**Что в .env:**
- Параметры подключения к БД (хост, порт, пользователь, пароль, название БД)
- API ключи (CHE168_API_KEY, CHE168_ACCESS_NAME)
- Секретные ключи для генерации токенов

**Что в config файле:**
- Настройки логирования (путь к файлам, уровни, формат)
- Настройки rate limiting
- Настройки пагинации
- Размеры батчей для обработки
- Интервалы повторных попыток
- Другие технические параметры

---

## Ответы на дополнительные вопросы (часть 3)

### 1. Обработка изменений в /changes - пагинация и пропущенные обновления

**Ответ:**
- **Пагинация:** Нужно обрабатывать все страницы изменений (используя `next_change_id` из `meta`)
- **Пропущенные обновления:** Берем дату последнего успешного обновления и начинаем со следующего дня
- **Получение change_id:** Используем `/change_id?date=...` для получения начального `change_id` по дате
- **Хранение:** Нужна отдельная таблица для хранения последнего обработанного `change_id` (см. таблицу `sync_state`)

**Логика работы:**
1. Определяем дату последнего успешного обновления из таблицы `sync_state` (поле `last_successful_date`)
2. Если пропущено несколько дней - обрабатываем все пропущенные дни в цикле:
   - Начинаем с даты `last_successful_date + 1 день`
   - Для каждой пропущенной даты до сегодняшней даты (включительно):
     a. Получаем начальный `change_id` через `/change_id?date=YYYY-MM-DD` для этой даты
     b. Обрабатываем все страницы через `/changes?change_id=...` пока `next_change_id` в `meta` не станет `null`
     c. После успешного завершения обработки даты обновляем `sync_state`: устанавливаем `last_successful_date` на обработанную дату и `last_change_id` на последний обработанный change_id
3. Если обновление было сегодня - обрабатываем только сегодняшнюю дату

---

### 2. Обработка change_type: "changed" - неполные данные

**Ответ:** При `change_type: "changed"` в `data` могут быть только измененные поля.

**Логика обновления:**
- Обновлять только измененные поля в JSON (делать merge с существующими данными)
- При маппинге полей: если в API приходит поле с префиксом `new_` (например, `new_price`), переименовывать его в соответствующее поле без префикса (`price`) при merge
- В БД хранить данные с оригинальными названиями ключей (без префиксов `new_`)
- Не перезаписывать весь JSON, а делать частичное обновление

**Пример:**
- Было: `data = {"price": 8000, "mark": "Kia", ...}`
- Пришло изменение: `{"new_price": 7500}`
- При merge: ищем `new_price`, переименовываем в `price`, обновляем значение
- Стало: `data = {"price": 7500, "mark": "Kia", ...}` (обновлен только price, остальные поля без изменений)

---

### 3. Обработка ошибок при нормализации

**Ответ:**
- Если нормализатор не смог обработать запись → **оставлять `is_processed = false`** (чтобы повторить попытку)
- **Логировать ошибки:** Да, нужно логировать ошибки нормализации для конкретных записей
- Если нормализатор упал на середине батча → **откатывать весь батч** (транзакция на весь батч)

---

### 4. Блокировка и параллельная обработка

**Ответ:**
- **Параллельная обработка:** Нет, процессы работают последовательно
- **Порядок:** Сначала скачивание данных, затем нормализация
- **Потоки:** Скачивание и обновление только в один поток (не параллельно)
- **Блокировка:** Не нужна, так как процессы работают последовательно

---

### 5. Валидация данных перед сохранением

**Ответ:**
- **Валидация JSON:** Не нужна
- Сохраняем данные как есть, без проверки структуры

---

### 6. Транзакционность при батчевой обработке

**Ответ:**
- При ошибке в батче из 200 записей → **откатывать весь батч**
- **Транзакция:** Да, нужна транзакция на весь батч
- **Обновление `is_processed`:** Обновлять в конце батча (после успешной обработки всех 200 записей)

---

### 7. Хранение последнего change_id

**Ответ:** Да, нужна отдельная таблица для хранения последнего обработанного `change_id`.

**Таблица:** `sync_state` (см. структуру БД ниже)

---

### 8. Обработка изображений

**Ответ:**
- **Скачивание:** Не нужно, сохранять изображения локально не требуется
- **Хранение:** Храним только URL из API в поле `data.images`

---

### 9. Кодировка и китайские символы

**Ответ:**
- **Кодировка БД:** UTF-8
- **Обработка символов:** Кодировку и китайские символы будем исправлять при нормализации
- Нормализация адресов и обработка смешанных данных будет в процессе нормализации

---

### 10. Производительность и оптимизация

**Ответ:**
- **Партиции:** Не нужны
- **Архивация старых данных:** Разберемся позже
- VACUUM/ANALYZE и материализованные представления - определим при необходимости

---

## Финальные уточнения (часть 4)

### 1. Обработка ошибок при первоначальной загрузке

**Ответ:** Для первоначальной загрузки использовать ту же стратегию повторных попыток, что и для ежедневных обновлений:
- **Повторные попытки:** 20 раз в течение 4 часов (интервал 12 минут между попытками)
- При ошибке на конкретной странице - повторять запрос этой страницы
- Если после всех попыток не удалось загрузить страницу - логировать ошибку и продолжать со следующей страницы

---

### 2. Обработка нескольких пропущенных дней

**Ответ:** Если пропущено несколько дней обновлений, обрабатывать все пропущенные дни в цикле:
- Начинать с даты `last_successful_date + 1 день`
- Обрабатывать каждый пропущенный день последовательно до сегодняшней даты (включительно)
- Для каждого дня получать `change_id` через `/change_id?date=...` и обрабатывать все страницы изменений
- После успешной обработки каждого дня обновлять `sync_state`

**Пример:**
- Последнее обновление: 2025-02-03
- Сегодня: 2025-02-06
- Обрабатываем: 2025-02-04, затем 2025-02-05, затем 2025-02-06

---

### 3. Маппинг полей при merge JSON (change_type: "changed")

**Ответ:** При merge измененных полей в JSON:
- Если в API приходит поле с префиксом `new_` (например, `new_price`), переименовывать его в соответствующее поле без префикса (`price`) при merge
- В БД хранить данные с оригинальными названиями ключей (без префиксов `new_`)
- Логика: искать `new_*` поля, маппить их в соответствующие поля без префикса, обновлять значения

**Пример маппинга:**
- `new_price` → `price`
- `new_mileage` → `mileage`
- И т.д.

---

### 4. Запуск ежедневных процессов

**Ответ:** Использовать Cron на VPS для запуска ежедневных процессов:
- Настроить Cron задачи для запуска процесса загрузки данных (один раз в сутки)
- Настроить Cron задачу для запуска процесса нормализации (после загрузки данных)
- Время запуска определить при настройке Cron

---

### 5. Генерация токенов

**Ответ:** Использовать UUID-подобный формат для генерации токенов:
- **Формат:** 16 символов (буквы и цифры)
- **Пример:** `a1b2c3d4e5f6g7h8` или `1a2b3c4d5e6f7g8h`
- **Алгоритм:** Генерировать случайную строку из 16 символов (буквы a-z, A-Z, цифры 0-9)

---

### 6. Формат конфигурационного файла

**Ответ:** Использовать YAML формат для конфигурационного файла:
- **Имя файла:** `config.yaml`
- **Содержимое:** Все технические настройки (логирование, rate limiting, размеры батчей, интервалы повторных попыток, таймауты и т.д.)

---

### 7. Формат прогресс-бара

**Ответ:** Использовать консольный текстовый формат для отображения прогресса:
- Выводить прогресс в консоль (stdout)
- Формат: процент выполнения, количество обработанных записей, скорость обработки и т.д.
- Пример: `[INFO] Progress: 45.2% (112,500 / 250,000 records) | Speed: 500 records/min`

---

# Задачи

## В работе


## Запланировано

### Этап 1: Планирование и уточнение требований
- ✅ Изучение API документации CHE168.COM
- ✅ Анализ структуры данных из API
- ✅ Уточнение требований к БД и обработке данных
- ✅ Определение логики работы с изменениями
- ✅ Планирование архитектуры системы
- ✅ Уточнение обработки ошибок и повторных попыток
- ✅ Уточнение логики обработки пропущенных дней
- ✅ Уточнение маппинга полей при merge JSON
- ✅ Уточнение форматов конфигурации и токенов
- ✅ Уточнение способа запуска процессов

**Статус:** Завершено. Все вопросы уточнены, можно приступать к реализации.

---

## Пошаговый план реализации

### Этап 2: Настройка инфраструктуры проекта

#### 2.1. Структура проекта
- ✅ Создать структуру директорий проекта:
  ```
  auto-parser-back-and-load/
  ├── app/
  │   ├── __init__.py
  │   ├── api/              # Публичное API
  │   ├── loaders/          # Модули загрузки данных
  │   ├── normalizers/      # Модули нормализации
  │   ├── database/         # Модели БД, подключение
  │   ├── utils/            # Утилиты (логирование, конфиг, retry)
  │   └── services/         # Бизнес-логика
  ├── alembic/              # Миграции БД
  ├── scripts/              # Скрипты для запуска
  ├── logs/                 # Директория для логов
  ├── config.yaml           # Конфигурация
  ├── .env                  # Переменные окружения
  ├── requirements.txt      # Зависимости
  ├── Dockerfile            # Docker образ
  ├── docker-compose.yml    # Docker Compose для VPS
  └── README.md
  ```

#### 2.2. Зависимости и окружение
- ✅ Обновить `requirements.txt`:
  - `fastapi` - API фреймворк
  - `uvicorn` - ASGI сервер
  - `sqlalchemy[asyncio]` - ORM с async поддержкой
  - `asyncpg` - async драйвер PostgreSQL
  - `alembic` - миграции БД
  - `pydantic` - валидация данных
  - `pydantic-settings` - настройки из env
  - `python-dotenv` - загрузка .env
  - `pyyaml` - парсинг YAML конфига
  - `requests` - HTTP клиент для CHE168 API
  - `python-multipart` - для FastAPI форм
- [ ] Создать виртуальное окружение
- [ ] Установить зависимости

#### 2.3. Конфигурация
- ✅ Создать `config.yaml` с настройками:
  - Логирование (путь, уровни, ротация)
  - Размеры батчей (200 для нормализации)
  - Интервалы повторных попыток (12 минут, 20 попыток)
  - Таймауты для API запросов
  - Rate limiting настройки
- [ ] Обновить `.env` с примерами всех переменных
- [ ] Создать модуль `app/utils/config.py` для загрузки конфигурации

#### 2.4. Система логирования
- ✅ Создать модуль `app/utils/logger.py`:
  - Настройка logging с уровнями INFO/WARNING/ERROR
  - Ротация логов (удаление старше недели)
  - Формат: `[LEVEL] message`
  - Запись в файлы в директории `logs/`
  - Логи на английском языке

---

### Этап 3: База данных и миграции

#### 3.1. Настройка Alembic
- ✅ Инициализировать Alembic: `alembic init alembic`
- ✅ Настроить `alembic/env.py` для async SQLAlchemy
- ✅ Настроить подключение к БД из `.env`

#### 3.2. Модели базы данных
- ✅ Создать `app/database/models.py` с моделями:
  - `RawData` - сырые данные из API
  - `ProcessedData` - нормализованные данные (базовая структура)
  - `ApiToken` - токены для API
  - `OperationsLog` - логи операций
  - `SyncState` - состояние синхронизации
- [ ] Определить все колонки, типы, ограничения
- [ ] Добавить индексы (inner_id, active_status, is_processed, и т.д.)

#### 3.3. Миграции
- ✅ Создать начальную миграцию: `alembic revision --autogenerate -m "Initial migration"`
- ✅ Проверить сгенерированную миграцию
- ✅ Применить миграцию: `alembic upgrade head` (готово к применению)

#### 3.4. Подключение к БД
- ✅ Создать `app/database/connection.py`:
  - Async engine для SQLAlchemy
  - Session factory
  - Функции для получения сессий

---

### Этап 4: Утилиты и вспомогательные модули

#### 4.1. Retry механизм
- ✅ Создать `app/utils/retry.py`:
  - Функция retry с настраиваемыми параметрами
  - Логика: 20 попыток в течение 4 часов (интервал 12 минут)
  - Логирование каждой попытки
  - Обработка различных типов исключений

#### 4.2. CHE168 API клиент
- ✅ Создать `app/utils/che168_client.py`:
  - Базовый класс для работы с CHE168 API
  - Методы для всех эндпоинтов:
    - `get_filters()` - получение фильтров
    - `get_offers(page)` - получение объявлений с пагинацией
    - `get_change_id(date)` - получение change_id по дате
    - `get_changes(change_id)` - получение изменений
    - `get_offer(inner_id)` - получение конкретного объявления
  - Обработка ошибок API
  - Retry механизм для всех запросов

#### 4.3. Прогресс-бар
- ✅ Создать `app/utils/progress.py`:
  - Консольный прогресс-бар
  - Отображение: процент, количество записей, скорость
  - Формат: `[INFO] Progress: 45.2% (112,500 / 250,000 records) | Speed: 500 records/min`

#### 4.4. Генерация токенов
- ✅ Создать `app/utils/token_generator.py`:
  - Функция генерации UUID-подобных токенов (16 символов)
  - Генерация 20 токенов при инициализации
  - Сохранение токенов в БД

---

### Этап 5: Модуль загрузки данных

#### 5.1. Базовый класс загрузчика
- ✅ Создать `app/loaders/base_loader.py`:
  - Базовый класс с общей логикой
  - Методы для работы с БД
  - Логирование операций
  - Запись в `operations_log`

#### 5.2. Первоначальная загрузка
- ✅ Создать `app/loaders/initial_loader.py`:
  - Класс `InitialLoader` для загрузки всех объявлений
  - Логика пагинации через `/offers?page=...`
  - Обработка `next_page` из `meta`
  - Сохранение данных в `raw_data`:
    - Извлечение `id`, `inner_id`, `change_type`, `created_at` из `result`
    - Сохранение `data` как JSONB
    - Установка `source = "initial_load"`
    - Установка `active_status = 0`
    - Установка `is_processed = false`
  - Обработка дубликатов (игнорирование если `inner_id` уже существует)
  - Retry механизм для каждой страницы
  - Прогресс-бар
  - Продолжение с последней страницы при прерывании

#### 5.3. Ежедневные обновления
- ✅ Создать `app/loaders/daily_updater.py`:
  - Класс `DailyUpdater` для ежедневных обновлений
  - Логика определения пропущенных дней из `sync_state`
  - Обработка всех пропущенных дней в цикле:
    - Получение `change_id` через `/change_id?date=...`
    - Пагинация через `/changes?change_id=...`
    - Обработка `next_change_id` из `meta`
  - Обработка разных `change_type`:
    - `"added"` → создание новой записи
    - `"changed"` → обновление существующей (merge JSON с маппингом `new_*` полей)
    - `"removed"` → обновление `active_status = 1`
  - Edge cases: создание записи если `inner_id` не найден
  - Обновление `sync_state` после успешной обработки каждого дня
  - Retry механизм
  - Логирование операций

#### 5.4. Merge JSON логика
- ✅ Создать `app/utils/json_merger.py`:
  - Функция для merge JSON с маппингом полей
  - Логика: `new_price` → `price`, `new_mileage` → `mileage` и т.д.
  - Сохранение оригинальных названий ключей в БД

#### 5.5. CLI скрипты для запуска
- ✅ Создать `scripts/initial_load.py`:
  - Скрипт для запуска первоначальной загрузки
  - Инициализация БД, конфига, логгера
  - Запуск `InitialLoader`
- ✅ Создать `scripts/daily_update.py`:
  - Скрипт для запуска ежедневных обновлений
  - Инициализация БД, конфига, логгера
  - Запуск `DailyUpdater`

---

### Этап 6: Модуль нормализации данных

#### 6.1. Базовый класс нормализатора
- ✅ Создать `app/normalizers/base_normalizer.py`:
  - Базовый класс с общей логикой
  - Методы для работы с БД
  - Логирование операций
  - Запись в `operations_log`

#### 6.2. Нормализатор данных
- ✅ Создать `app/normalizers/data_normalizer.py`:
  - Класс `DataNormalizer` для нормализации данных
  - Выборка записей где `is_processed = false`
  - Обработка батчами по 200 записей (настраивается через config или аргумент)
  - Транзакция на весь батч (откат при любой ошибке)
  - Логика нормализации:
    - Извлечение основных полей из JSON `data` (mark, model, year, price, km_age, и т.д.)
    - Преобразование строковых чисел в числовые типы
    - Парсинг JSON строк (например, images)
    - Извлечение ключевых параметров из configuration (specid, car_name, manufacturer, и т.д.)
    - Сохранение нормализованных данных в `processed_data.normalized_data` (JSONB)
  - Обновление `is_processed = true` в конце успешного батча
  - Откат транзакции при ошибке в батче (весь батч остается необработанным)
  - Логирование ошибок для конкретных записей
  - Продолжение с места остановки при падении (записи с `is_processed = false` обрабатываются заново)
  - Прогресс-бар для отображения прогресса нормализации

#### 6.3. CLI скрипт для нормализации
- ✅ Создать `scripts/normalize.py`:
  - Скрипт для запуска нормализации
  - Инициализация БД, конфига, логгера
  - Запуск `DataNormalizer`
  - Поддержка аргумента `--batch-size` для настройки размера батча
  - Вывод статистики после завершения

---

### Этап 7: Публичное API

#### 7.1. Аутентификация
- [ ] Создать `app/api/auth.py`:
  - Middleware для проверки токенов
  - Валидация токена из заголовка
  - Обновление `last_used_at` при использовании токена
  - Обработка невалидных/истекших токенов

#### 7.2. Rate limiting
- [ ] Создать `app/api/rate_limit.py`:
  - Middleware для rate limiting
  - Одинаковые лимиты для всех токенов
  - Логирование превышения лимитов

#### 7.3. Эндпоинты API
- [ ] Создать `app/api/routes.py`:
  - `GET /offers` - список объявлений:
    - Фильтрация (по марке, модели, цене и т.д.)
    - Пагинация
    - Только активные объявления (`active_status = 0`)
    - Данные из `processed_data`
  - `GET /offers/{inner_id}` - детали конкретного объявления:
    - Данные из `processed_data`
    - Обработка несуществующих объявлений
  - `GET /health` - health check (опционально)

#### 7.4. Pydantic схемы
- [ ] Создать `app/api/schemas.py`:
  - Схемы запросов (фильтры, пагинация)
  - Схемы ответов (список объявлений, детали объявления)

#### 7.5. Главное приложение FastAPI
- [ ] Создать `app/api/main.py`:
  - Инициализация FastAPI приложения
  - Подключение роутов
  - Подключение middleware (auth, rate limiting)
  - Обработка ошибок
  - CORS настройки (если нужно)

#### 7.6. Запуск API сервера
- [ ] Создать `scripts/run_api.py`:
  - Скрипт для запуска FastAPI сервера
  - Использование uvicorn
  - Настройки хоста и порта из конфига

---

### Этап 8: Инициализация и настройка

#### 8.1. Генерация токенов
- [ ] Создать `scripts/generate_tokens.py`:
  - Скрипт для генерации 20 токенов
  - Сохранение токенов в БД
  - Вывод токенов в консоль для ручного распределения

#### 8.2. Инициализация БД
- [ ] Создать `scripts/init_db.py`:
  - Скрипт для первоначальной настройки БД
  - Применение миграций
  - Создание начальных записей (если нужно)

---

### Этап 9: Тестирование (локально)

#### 9.1. Тестирование загрузки данных
- [ ] Протестировать первоначальную загрузку на небольшом объеме данных
- [ ] Проверить обработку ошибок и retry механизм
- [ ] Проверить прогресс-бар
- [ ] Проверить продолжение с последней страницы

#### 9.2. Тестирование ежедневных обновлений
- [ ] Протестировать получение изменений
- [ ] Проверить обработку разных `change_type`
- [ ] Проверить merge JSON с маппингом полей
- [ ] Проверить обработку пропущенных дней

#### 9.3. Тестирование нормализации
- [ ] Протестировать нормализацию батчами
- [ ] Проверить транзакционность
- [ ] Проверить обработку ошибок

#### 9.4. Тестирование API
- [ ] Протестировать все эндпоинты
- [ ] Проверить аутентификацию
- [ ] Проверить rate limiting
- [ ] Проверить фильтрацию и пагинацию

---

### Этап 10: Docker и развертывание

#### 10.1. Dockerfile
- [ ] Создать `Dockerfile`:
  - Базовый образ Python
  - Установка зависимостей
  - Копирование кода
  - Настройка рабочей директории
  - Команда запуска

#### 10.2. Docker Compose
- [ ] Создать `docker-compose.yml`:
  - Сервис для API
  - Сервис для PostgreSQL
  - Настройка сетей и volumes
  - Переменные окружения

#### 10.3. Репликация БД на VPS
- [ ] Создать скрипт `scripts/dump_db.sh`:
  - Экспорт БД через `pg_dump`
  - Создание дампа для переноса
- [ ] Создать скрипт `scripts/restore_db.sh`:
  - Импорт БД через `pg_restore`
  - Восстановление на VPS

#### 10.4. Настройка Cron на VPS
- [ ] Создать файл с Cron задачами:
  - Ежедневный запуск `daily_update.py` (например, в 02:00)
  - Ежедневный запуск `normalize.py` (например, в 03:00, после загрузки)
- [ ] Документация по настройке Cron

#### 10.5. Документация развертывания
- [ ] Создать `DEPLOYMENT.md`:
  - Инструкции по развертыванию на VPS
  - Настройка Docker
  - Настройка Cron
  - Мониторинг и логи

---

### Этап 11: Полная первоначальная загрузка

#### 11.1. Локальная загрузка
- [ ] Запустить полную первоначальную загрузку (~250k объявлений) локально
- [ ] Мониторинг процесса
- [ ] Проверка данных в БД
- [ ] Обработка ошибок и прерываний

#### 11.2. Репликация на VPS
- [ ] Создать дамп БД локально
- [ ] Перенести дамп на VPS
- [ ] Восстановить БД на VPS
- [ ] Проверить целостность данных

---

### Этап 12: Финальная настройка и запуск

#### 12.1. Запуск на VPS
- [ ] Развернуть Docker контейнеры на VPS
- [ ] Настроить переменные окружения
- [ ] Запустить API сервер
- [ ] Проверить доступность API

#### 12.2. Настройка Cron
- [ ] Настроить Cron задачи на VPS
- [ ] Протестировать запуск ежедневных процессов
- [ ] Проверить логи

#### 12.3. Мониторинг
- [ ] Настроить мониторинг логов
- [ ] Проверить работу системы в течение нескольких дней
- [ ] Обработка возникающих проблем

---

## Порядок выполнения этапов

**Последовательность:**
1. Этап 2 → Этап 3 → Этап 4 → Этап 5 → Этап 6 → Этап 7 → Этап 8 → Этап 9 → Этап 10 → Этап 11 → Этап 12

**Параллельно можно выполнять:**
- Разработка модулей загрузки (Этап 5) и нормализации (Этап 6) может идти параллельно после завершения Этапа 4
- Разработка API (Этап 7) может начаться параллельно с Этапом 6

**Критический путь:**
- Этап 2 (инфраструктура) → Этап 3 (БД) → Этап 4 (утилиты) → Этап 5 (загрузка) → Этап 11 (полная загрузка) → Этап 12 (развертывание)

## Выполнено

- ✅ Создание документации API (`CHE168_API_DOCUMENTATION.md`)
- ✅ Создание справочника фильтров (`get_filters.py`, `filters_reference.py`)
- ✅ Планирование структуры БД и логики обработки данных
- ✅ Уточнение всех технических деталей проекта

---

## Реализованные модули

### Этап 2: Настройка инфраструктуры проекта ✅**Структура проекта:**
- Создана полная структура директорий согласно плану
- Все необходимые модули и директории на месте
- Отсутствуют только Docker файлы (будут созданы на Этапе 10)**Зависимости:**
- `requirements.txt` содержит все необходимые пакеты:
  - FastAPI, uvicorn для API
  - SQLAlchemy[asyncio], asyncpg для работы с БД
  - Alembic для миграций
  - Pydantic для валидации
  - PyYAML для конфигурации
  - requests для работы с CHE168 API

**Конфигурация:**
- `config.yaml` содержит все настройки:
  - Логирование (уровни, ротация, retention 7 дней)
  - Размеры батчей (200 для нормализации)
  - Retry настройки (20 попыток, 12 минут интервал, 4 часа таймаут)
  - API таймауты (30 секунд)
  - Rate limiting настройки
  - Пагинация
  - CHE168 API endpoints
- Модуль `app/utils/config.py` с классами `Config` и `EnvConfig`

**Система логирования:**
- Модуль `app/utils/logger.py`:
  - Настройка logging с уровнями (DEBUG, INFO, WARNING, ERROR)
  - Ротация логов через RotatingFileHandler (10 MB, 5 backup файлов)
  - Автоматическое удаление логов старше 7 дней
  - Формат: `[LEVEL] message`
  - Запись в `logs/app.log`
  - Логи на английском языке

---

### Этап 3: База данных и миграции ✅

**Настройка Alembic:**
- Alembic инициализирован и настроен для async SQLAlchemy
- `alembic/env.py` настроен для работы с asyncpg
- Подключение к БД из `.env` настроено

**Модели базы данных:**
- `app/database/models.py` содержит все модели:
  - `RawData` - сырые данные из API (все колонки и индексы)
  - `ProcessedData` - нормализованные данные (базовая структура с JSONB)
  - `ApiToken` - токены для API (16 символов)
  - `OperationsLog` - логи операций (с индексами)
  - `SyncState` - состояние синхронизации (для отслеживания последнего обновления)

**Миграции:**
- Создана начальная миграция `9e5b5b4d7461_initial_migration.py`
- Миграция включает все таблицы, колонки, индексы и ограничения
- Готова к применению через `alembic upgrade head`

**Подключение к БД:**
- `app/database/connection.py`:
  - Async engine для SQLAlchemy с настройками пула соединений
  - AsyncSessionLocal factory
  - Функции `get_db()`, `init_db()`, `close_db()`

---

### Этап 4: Утилиты и вспомогательные модули ✅

**Retry механизм:**
- `app/utils/retry.py`:
  - Функции `retry_async()` и `retry_sync()`
  - Настраиваемые параметры (max_attempts, interval, timeout)
  - Логика: 20 попыток в течение 4 часов (интервал 12 минут)
  - Логирование каждой попытки
  - Обработка таймаутов

**CHE168 API клиент:**
- `app/utils/che168_client.py`:
  - Класс `CHE168Client` для работы с CHE168 API
  - Методы для всех эндпоинтов:
    - `get_filters()` - получение фильтров
    - `get_offers(page, ...)` - получение объявлений с пагинацией и фильтрами
    - `get_change_id(date)` - получение change_id по дате
    - `get_changes(change_id)` - получение изменений
    - `get_offer(inner_id)` - получение конкретного объявления
  - Retry механизм для всех запросов
  - Обработка ошибок API

**Прогресс-бар:**
- `app/utils/progress.py`:
  - Класс `ProgressBar` с консольным выводом
  - Отображение: процент, количество записей, скорость (records/min), ETA
  - Формат соответствует требованиям: `[INFO] Progress: 45.2% (112,500 / 250,000 records) | Speed: 500 records/min`

**Генерация токенов:**
- `app/utils/token_generator.py`:
  - Функция `generate_token()` - UUID-подобные токены (16 символов, буквы и цифры)
  - Функция `generate_tokens()` - генерация нескольких уникальных токенов
  - Функция `validate_token()` - валидация формата токена

---

### Этап 5: Модуль загрузки данных ✅

**Базовый класс загрузчика:**
- `app/loaders/base_loader.py`:
  - Класс `BaseLoader` с общей логикой
  - Методы: `start_operation()`, `finish_operation()`, `record_error()`
  - Запись в `operations_log` с автоматическим определением статуса

**Первоначальная загрузка:**
- `app/loaders/initial_loader.py`:
  - Класс `InitialLoader` для загрузки всех объявлений
  - Пагинация через `/offers?page=...` с обработкой `next_page` из `meta`
  - Сохранение данных в `raw_data`:
    - Извлечение `id`, `inner_id`, `change_type`, `created_at` из `result`
    - Сохранение `data` как JSONB
    - Установка `source = "initial_load"`, `active_status = 0`, `is_processed = false`
  - Обработка дубликатов (игнорирование если `inner_id` уже существует)
  - Retry механизм для каждой страницы
  - Логирование операций
  - Поддержка `max_pages` для тестирования

**Ежедневные обновления:**
- `app/loaders/daily_updater.py`:
  - Класс `DailyUpdater` для ежедневных обновлений
  - Логика определения пропущенных дней из `sync_state`
  - Обработка всех пропущенных дней в цикле:
    - Получение `change_id` через `/change_id?date=...`
    - Пагинация через `/changes?change_id=...` с обработкой `next_change_id`
  - Обработка разных `change_type`:
    - `"added"` → создание новой записи
    - `"changed"` → обновление существующей (merge JSON с маппингом `new_*` полей)
    - `"removed"` → обновление `active_status = 1`
  - Edge cases: создание записи если `inner_id` не найден
  - Обновление `sync_state` после успешной обработки каждого дня
  - Retry механизм
  - Логирование операций
  - Поддержка `max_dates` для тестирования

**Merge JSON логика:**
- `app/utils/json_merger.py`:
  - Функция `merge_json()` - merge с маппингом полей
  - Логика: `new_price` → `price`, `new_mileage` → `mileage` и т.д.
  - Функция `map_new_fields()` для обработки изменений

**CLI скрипты:**
- `scripts/initial_load.py` - скрипт для первоначальной загрузки с поддержкой `--max-pages`
- `scripts/daily_update.py` - скрипт для ежедневных обновлений с поддержкой `--max-dates`

---

### Этап 6: Модуль нормализации данных ✅

**Базовый класс нормализатора:**
- `app/normalizers/base_normalizer.py`:
  - Класс `BaseNormalizer` с общей логикой
  - Методы: `start_operation()`, `finish_operation()`, `record_error()`
  - Запись в `operations_log` с автоматическим определением статуса

**Нормализатор данных:**
- `app/normalizers/data_normalizer.py`:
  - Класс `DataNormalizer` для нормализации данных
  - Выборка записей где `is_processed = false`
  - Обработка батчами по 200 записей (настраивается через config или аргумент `--batch-size`)
  - Транзакция на весь батч (откат при любой ошибке в батче)
  - Логика нормализации:
    - Извлечение основных полей из JSON `data`:
      - Базовые поля: `inner_id`, `url`, `mark`, `model`, `year`, `color`, `price`, `km_age`
      - Технические характеристики: `engine_type`, `transmission_type`, `body_type`, `displacement`, `power`
      - Информация о продавце: `address`, `seller_type`, `is_dealer`, `section`, `salon_id`
      - Дополнительные поля: `description`, `vin`, `offer_created`, `first_registration`, `drive_type`
    - Преобразование типов:
      - Строковые числа в числовые типы (year, price, km_age, power → int; displacement → float)
      - Строковые булевы значения в boolean (`"true"` → `True`)
      - Парсинг JSON строк (например, `images` из строки в массив)
    - Извлечение ключевых параметров из `configuration`:
      - `specid` - ID спецификации
      - Ключевые параметры из `paramtypeitems`:
        - Из "基本参数": car_name, manufacturer, car_class, fuel_type, emission_standard
        - Из "发动机": engine_model, displacement_l, max_horsepower, max_power_kw
        - Из "变速箱": transmission_type_detail, gears
    - Сохранение нормализованных данных в `processed_data.normalized_data` (JSONB)
  - Обновление `is_processed = true` в конце успешного батча
  - Откат транзакции при ошибке в батче (весь батч остается необработанным)
  - Логирование ошибок для конкретных записей
  - Продолжение с места остановки при падении (записи с `is_processed = false` обрабатываются заново)
  - Прогресс-бар для отображения прогресса нормализации
  - Статистика: total_processed, total_created, total_updated, total_errors, total_batches

**CLI скрипт:**
- `scripts/normalize.py`:
  - Скрипт для запуска нормализации
  - Инициализация БД, конфига, логгера
  - Запуск `DataNormalizer`
  - Поддержка аргумента `--batch-size` для настройки размера батча
  - Вывод статистики после завершения---

## Статус реализации

**Выполнено:**
- ✅ Этап 1: Планирование и уточнение требований
- ✅ Этап 2: Настройка инфраструктуры проекта
- ✅ Этап 3: База данных и миграции
- ✅ Этап 4: Утилиты и вспомогательные модули
- ✅ Этап 5: Модуль загрузки данных
- ✅ Этап 6: Модуль нормализации данных

**В работе:**
- Нет

**Запланировано:**
- Этап 7: Публичное API
- Этап 8: Инициализация и настройка (частично - генерация токенов)
- Этап 9: Тестирование
- Этап 10: Docker и развертывание
- Этап 11: Полная первоначальная загрузка
- Этап 12: Финальная настройка и запуск

**Прогресс:** ~60% инфраструктуры и обработки данных реализовано

---

## Структура проекта

```
auto-parser-back-and-load/
├── app/
│   ├── __init__.py
│   ├── api/                      # Публичное API (будет реализовано)
│   │   └── __init__.py
│   ├── loaders/                  # Модули загрузки данных ✅
│   │   ├── __init__.py
│   │   ├── base_loader.py       # Базовый класс загрузчика
│   │   ├── initial_loader.py    # Первоначальная загрузка
│   │   └── daily_updater.py    # Ежедневные обновления
│   ├── normalizers/              # Модули нормализации ✅
│   │   ├── __init__.py
│   │   ├── base_normalizer.py  # Базовый класс нормализатора
│   │   └── data_normalizer.py  # Нормализатор данных
│   ├── database/                 # Модели БД, подключение ✅
│   │   ├── __init__.py
│   │   ├── models.py            # Модели SQLAlchemy
│   │   └── connection.py        # Подключение к БД
│   ├── utils/                    # Утилиты ✅
│   │   ├── __init__.py
│   │   ├── config.py            # Загрузка конфигурации
│   │   ├── logger.py            # Система логирования
│   │   ├── retry.py             # Retry механизм
│   │   ├── che168_client.py     # Клиент CHE168 API
│   │   ├── progress.py          # Прогресс-бар
│   │   ├── json_merger.py       # Merge JSON логика
│   │   └── token_generator.py   # Генерация токенов
│   └── services/                 # Бизнес-логика (зарезервировано)
│       └── __init__.py
├── alembic/                      # Миграции БД ✅
│   ├── env.py                   # Настройка Alembic
│   ├── versions/
│   │   └── 9e5b5b4d7461_initial_migration.py
│   └── script.py.mako
├── scripts/                      # Скрипты для запуска ✅
│   ├── initial_load.py          # Первоначальная загрузка
│   ├── daily_update.py          # Ежедневные обновления
│   ├── normalize.py             # Нормализация данных
│   └── test_db_connection.py    # Тест подключения к БД
├── logs/                         # Директория для логов
│   └── app.log
├── config.yaml                   # Конфигурация ✅
├── .env                          # Переменные окружения
├── requirements.txt              # Зависимости ✅
├── alembic.ini                   # Конфигурация Alembic
├── README.md                     # Основная документация
├── TASKS.md                      # Этот файл - задачи и статус
├── CHE168_API_DOCUMENTATION.md  # Документация API
├── example.txt                   # Пример данных из API
├── get_filters.py                # Скрипт получения фильтров
├── filters_reference.py          # Справочник фильтров
├── filters_reference.json        # JSON справочник фильтров
└── filters_full_response.json    # Полный ответ API фильтров
```

---

## Инструкции по использованию

### Настройка окружения

1. **Установка зависимостей:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Настройка переменных окружения (.env):**
   ```env
   DB_HOST=localhost
   DB_PORT=5432
   DB_USER=postgres
   DB_PASSWORD=ваш_пароль
   DB_NAME=che168_db
   CHE168_API_KEY=ваш_api_ключ
   CHE168_ACCESS_NAME=autobase
   SECRET_KEY=секретный_ключ_для_токенов
   API_HOST=0.0.0.0
   API_PORT=8000
   ```

3. **Применение миграций БД:**
   ```bash
   alembic upgrade head
   ```

### Запуск модулей

#### Первоначальная загрузка данных
```bash
# Загрузка всех данных (может занять много времени)
python scripts/initial_load.py

# Тестовая загрузка (только первые 10 страниц)
python scripts/initial_load.py --max-pages 10
```

#### Ежедневные обновления
```bash
# Обновление всех пропущенных дней
python scripts/daily_update.py

# Тестовое обновление (только 1 день)
python scripts/daily_update.py --max-dates 1
```

#### Нормализация данных
```bash
# Нормализация всех необработанных записей
python scripts/normalize.py

# Нормализация с кастомным размером батча
python scripts/normalize.py --batch-size 100
```

### Мониторинг

- **Логи:** Все операции логируются в `logs/app.log`
- **Операции:** История операций хранится в таблице `operations_log`
- **Прогресс:** Прогресс-бары отображаются в консоли при выполнении операций

### Структура базы данных

**Таблицы:**
- `raw_data` - сырые данные из API (JSONB)
- `processed_data` - нормализованные данные (JSONB)
- `api_tokens` - токены для доступа к API
- `operations_log` - логи операций
- `sync_state` - состояние синхронизации

Подробное описание структуры БД см. в разделе "Итоговая структура базы данных" выше.
