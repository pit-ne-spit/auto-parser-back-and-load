# Модуль единоразового скачивания данных

## Описание

Модуль `InitialLoader` предназначен для первоначальной загрузки всех объявлений из API CHE168.COM. Этот модуль используется один раз для заполнения базы данных всеми доступными объявлениями (около 250,000 записей).

## Расположение

- **Класс:** `app.loaders.initial_loader.InitialLoader`
- **Скрипт запуска:** `scripts/initial_load.py`
- **Базовый класс:** `app.loaders.base_loader.BaseLoader`

## Логика работы

### 1. Инициализация

```python
loader = InitialLoader(max_pages=None)
```

**Параметры:**
- `max_pages` (Optional[int]): Максимальное количество страниц для загрузки. Если `None`, загружаются все страницы. Используется для тестирования.

### 2. Процесс загрузки

#### Шаг 1: Получение данных через API

1. Начинается с первой страницы (`page = 1`)
2. Для каждой страницы выполняется запрос к `/offers?page={page}`
3. Используется rate limiting: пауза 0.7 секунды между запросами
4. Запросы выполняются через `asyncio.to_thread()` для избежания блокировки event loop

#### Шаг 2: Обработка ответа

1. Извлекается массив `result` с объявлениями
2. Из `meta` извлекается `next_page` для определения наличия следующей страницы
3. Если `next_page` равен `None`, загрузка завершается

#### Шаг 3: Сохранение в базу данных

Для каждого объявления из `result`:

1. **Проверка дубликатов:**
   - Проверяется наличие записи с таким же `inner_id` в таблице `raw_data`
   - Если запись существует, она пропускается (дубликат)

2. **Создание новой записи:**
   - `inner_id` - ID объявления на платформе (уникальный идентификатор)
   - `change_type` - тип изменения (обычно "added")
   - `created_at` - дата создания из API (парсится из ISO формата)
   - `data` - полные данные объявления в формате JSONB
   - `first_loaded_at` - текущее время UTC (когда мы впервые получили объявление)
   - `last_updated_at` - текущее время UTC
   - `source` - "initial_load" (источник данных)
   - `active_status` - 0 (активное объявление)
   - `is_processed` - False (требует нормализации)

3. **Коммит транзакции:**
   - Все записи со страницы сохраняются в одной транзакции
   - При ошибке коммита выполняется rollback

#### Шаг 4: Обработка ошибок

1. **Ошибки на уровне страницы:**
   - Логируются через `record_error()`
   - Увеличивается счетчик ошибок
   - Выполняется пауза 0.7 секунды
   - Переход к следующей странице
   - Если ошибок больше 10, процесс останавливается

2. **Ошибки на уровне записи:**
   - Логируются с указанием `inner_id`
   - Запись пропускается
   - Увеличивается счетчик пропущенных записей

#### Шаг 5: Отображение прогресса

В консоль выводится прогресс в формате:
```
[INFO] Initial load: Page 123 | Records loaded: 2,460 | Skipped: 5 | Errors: 0
```

### 3. Завершение работы

1. **Запись в operations_log:**
   - `operation_type` = "data_fetch"
   - `started_at` - время начала операции
   - `duration` - продолжительность в секундах
   - `status` - "OK" или "ERROR" (если были ошибки)
   - `details` - информация об ошибках (если были)

2. **Статистика:**
   - `total_pages` - количество обработанных страниц
   - `total_loaded` - количество загруженных записей
   - `total_skipped` - количество пропущенных записей (дубликаты)
   - `total_errors` - количество ошибок

## Использование

### Базовый запуск

```bash
python scripts/initial_load.py
```

Загружает все доступные страницы.

### Тестовый запуск (ограниченное количество страниц)

```bash
python scripts/initial_load.py --max-pages 10
```

Загружает только первые 10 страниц.

## Особенности реализации

### Rate Limiting

- Пауза 0.7 секунды между запросами к API
- Предотвращает превышение лимитов API

### Асинхронность

- Используется `asyncio.to_thread()` для выполнения синхронных HTTP запросов
- Не блокирует event loop
- Позволяет эффективно обрабатывать большие объемы данных

### Обработка дубликатов

- Проверка выполняется по `inner_id` (уникальный идентификатор)
- Дубликаты пропускаются без ошибок
- Это важно при повторном запуске после прерывания

### Продолжение после прерывания

- Модуль может быть прерван (Ctrl+C) и запущен заново
- Дубликаты автоматически пропускаются
- Можно продолжить с того места, где остановились

## Зависимости

- `CHE168Client` - клиент для работы с API
- `BaseLoader` - базовый класс с общей логикой
- `RawData` - модель базы данных
- `AsyncSessionLocal` - сессия базы данных
- `logger` - система логирования

## Логирование

Все операции логируются в `logs/app.log`:
- Начало и завершение операции
- Прогресс по страницам
- Ошибки с контекстом
- Статистика завершения

## Ограничения

1. **Размер страницы:** API всегда возвращает 20 записей на страницу (неизменяемо)
2. **Общее количество страниц:** Неизвестно заранее, определяется по `next_page`
3. **Время выполнения:** Для 250,000 записей (12,500 страниц) при паузе 0.7 сек потребуется около 2.5 часов

## Пример вывода

```
[INFO] Starting initial load (max_pages=None)
[INFO] Initial load: Page 1 | Records loaded: 20 | Skipped: 0 | Errors: 0
[INFO] Initial load: Page 2 | Records loaded: 40 | Skipped: 0 | Errors: 0
...
[INFO] Initial load completed: pages=12500, loaded=250000, skipped=0, errors=0
```
